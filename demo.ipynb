{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio.v3 as io\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from demo_utils import ( \n",
    "    plot_imgs, \n",
    "    compute_relative_pose, \n",
    "    plot_imgs_and_kpts, \n",
    "    compute_fundamental_from_relative_motion\n",
    ")\n",
    "\n",
    "from matchers.mnn import MNN\n",
    "matcher = MNN(min_score=0.5, ratio_test=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "down_scaling_factor = 2 # 4K: 1, QHD: 1.5, FHD: 2, HD: 3\n",
    "max_kpts = 2048\n",
    "\n",
    "from wrappers_manager import wrappers_manager\n",
    "\n",
    "wrapper_engine = 'disk-kornia'  # 'superpoint', 'disk', 'dedode', 'aliked'\n",
    "wrapper = wrappers_manager(wrapper_engine, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load images\n",
    "base_path = Path('assets') \n",
    "\n",
    "im_A_path = '26/IMG_4057_frame_000001.jpg'\n",
    "im_B_path = '13/IMG_5058_frame_000003.jpg'\n",
    "\n",
    "img_A = wrapper.load_image(str(base_path / 'frames' / im_A_path), scaling=down_scaling_factor)\n",
    "img_B = wrapper.load_image(str(base_path / 'frames' / im_B_path), scaling=down_scaling_factor)\n",
    "\n",
    "plot_imgs([img_A, img_B])\n",
    "print(f'Image A shape: {img_A.shape}, Image B shape: {img_B.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pose \n",
    "poses   = np.load(base_path / 'cameras/poses.npz', allow_pickle=True)\n",
    "cameras = np.load(base_path / 'cameras/cameras.npz', allow_pickle=True)\n",
    "pose_A, cam_id_A = poses[im_A_path].item()['P'], poses[im_A_path].item()['camera_id']\n",
    "pose_B, cam_id_B = poses[im_B_path].item()['P'], poses[im_B_path].item()['camera_id']\n",
    "\n",
    "K_A = cameras[str(cam_id_A)].item()['K']\n",
    "K_B = cameras[str(cam_id_B)].item()['K']\n",
    "R_A, t_A = pose_A[:3,:3], pose_A[:3,3]\n",
    "R_B, t_B = pose_B[:3,:3], pose_B[:3,3]\n",
    "\n",
    "R, t = compute_relative_pose(R_A, t_A, R_B, t_B)\n",
    "P = np.hstack((R, t.reshape(3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional image down scaling\n",
    "if down_scaling_factor != 1:\n",
    "    K_A = K_A.copy()\n",
    "    K_B = K_B.copy()\n",
    "    K_A[:2, :] /= down_scaling_factor\n",
    "    K_B[:2, :] /= down_scaling_factor\n",
    "\n",
    "F_gt = compute_fundamental_from_relative_motion(R, t, K_A, K_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    out1 = wrapper.extract(img_A.to(device), max_kpts=max_kpts)\n",
    "    out2 = wrapper.extract(img_B.to(device), max_kpts=max_kpts)\n",
    "\n",
    "kpts1, des1 = out1.kpts, out1.des\n",
    "kpts2, des2 = out2.kpts, out2.des\n",
    "print(kpts1.shape, des1.shape, kpts2.shape, des2.shape)\n",
    "\n",
    "matches = matcher.match([des1], [des2])[0].matches\n",
    "kpts1_matched = kpts1[matches[:, 0]].cpu().numpy()\n",
    "kpts2_matched = kpts2[matches[:, 1]].cpu().numpy()\n",
    "print('matched:', kpts1_matched.shape, kpts2_matched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.benchmark_utils import estimate_pose, compute_pose_error\n",
    "\n",
    "norm_threshold = 0.5 / (np.mean(np.abs(K_A[:2, :2])) + np.mean(np.abs(K_B[:2, :2])))\n",
    "R_est, t_est, mask = estimate_pose(\n",
    "    kpts1_matched, kpts2_matched,\n",
    "    K_A, K_B, norm_threshold\n",
    ")\n",
    "\n",
    "pose_err = compute_pose_error(P, R_est, t_est)\n",
    "print(f'R error: {pose_err[0]:.2f}, t error: {pose_err[1]:.2f}')\n",
    "\n",
    "plot_imgs_and_kpts(img_A.permute(1, 2, 0).cpu().numpy()*255, img_B.permute(1, 2, 0).cpu().numpy()*255, \n",
    "                kpts1_matched[mask], kpts2_matched[mask], # keeping only geometrically verified matches\n",
    "                scatter=False, F_gt=F_gt, reth=5,\n",
    "                highlight_bad_matches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction with SANDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'sandesc_models/{wrapper_engine.split(\"-\")[0]}/final.pth'\n",
    "\n",
    "weights = torch.load(path,  map_location=device, weights_only=False)\n",
    "config = weights['config']\n",
    "model_config = {'ch_in': config['model_config']['unet_ch_in'],\n",
    "                'kernel_size': config['model_config']['unet_kernel_size'],\n",
    "                'activ': config['model_config']['unet_activ'],\n",
    "                'norm': config['model_config']['unet_norm'],\n",
    "                'skip_connection': config['model_config']['unet_with_skip_connections'],\n",
    "                'spatial_attention': config['model_config']['unet_spatial_attention'],\n",
    "                'third_block': config['model_config']['third_block'],\n",
    "                }\n",
    "\n",
    "from sandesc_models.sandesc.network_descriptor import SANDesc\n",
    "network = SANDesc(**model_config).eval().to(device)\n",
    "\n",
    "network.load_state_dict(weights['state_dict'])\n",
    "wrapper.descriptor_network = network\n",
    "wrapper.add_custom_descriptor(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    out1 = wrapper.extract(img_A, max_kpts=max_kpts)\n",
    "    out2 = wrapper.extract(img_B, max_kpts=max_kpts)\n",
    "\n",
    "kpts1, des1 = out1.kpts, out1.des\n",
    "kpts2, des2 = out2.kpts, out2.des\n",
    "print(kpts1.shape, des1.shape, kpts2.shape, des2.shape)\n",
    "\n",
    "matches = matcher.match([des1], [des2])[0].matches\n",
    "kpts1_matched = kpts1[matches[:, 0]].cpu().numpy()\n",
    "kpts2_matched = kpts2[matches[:, 1]].cpu().numpy()\n",
    "print(kpts1_matched.shape, kpts2_matched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_threshold = 0.5 / (np.mean(np.abs(K_A[:2, :2])) + np.mean(np.abs(K_B[:2, :2])))\n",
    "R_est, t_est, mask = estimate_pose(\n",
    "    kpts1_matched, kpts2_matched,\n",
    "    K_A, K_B, norm_threshold\n",
    ")\n",
    "\n",
    "pose_err = compute_pose_error(P, R_est, t_est)\n",
    "print(f'R error: {pose_err[0]:.2f}, t error: {pose_err[1]:.2f}')\n",
    "\n",
    "plot_imgs_and_kpts(img_A_np, img_B_np, \n",
    "                kpts1_matched[mask], kpts2_matched[mask], # keeping only geometrically verified matches\n",
    "                scatter=False, F_gt=F_gt, reth=5,\n",
    "                highlight_bad_matches=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keypoint_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
